[{"authors":["admin"],"categories":null,"content":"I am a data scientist at Callisto Media, a data-driven book publishing company based in the San Francisco Bay Area. I identify product opportunities and develop predictive models, KPIs, research workflows, and data infrastructure.\nPreviously, I was a PhD student in Political Science at the University of California, Berkeley, where I conducted research on topics ranging from \u0026ldquo;small world\u0026rdquo; network dynamics, campaign finance networks, and prejudice against Muslim-Americans.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dnield.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a data scientist at Callisto Media, a data-driven book publishing company based in the San Francisco Bay Area. I identify product opportunities and develop predictive models, KPIs, research workflows, and data infrastructure.\nPreviously, I was a PhD student in Political Science at the University of California, Berkeley, where I conducted research on topics ranging from \u0026ldquo;small world\u0026rdquo; network dynamics, campaign finance networks, and prejudice against Muslim-Americans.","tags":null,"title":"David Nield","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://dnield.com/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://dnield.com/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://dnield.com/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://dnield.com/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["Data Science"],"content":"  Introduction Stratified training/test splits Data pre-processing Modeling Model evaluation Model tuning   Introduction This January I was fortunate enough to attend rstudio::conf(2020), the official conference hosted by RStudio, PBC creators of the RStudio IDE and major contributors to the {tidyverse} ecosystem that has become the defacto standard for data importation, manipulation, and visualization in R.\nI could not have asked for a better time for this conference to land in my back yard (San Francisco). As of this writing, I am only 7 months removed from graduating with my Masters. My graduate training left me with a deep understanding of linear models and design-based causal inference, but with little or no training in other types of predictive modeling, unsupervised machine learning, version control, or putting models into production.\nBeing able to attend this conference in my first year as a data professional is an enormous blessing and I thank all of the workshop leaders, TAs, session presenters, and conference attendees for creating such a welcoming environment. Every interaction I had at the conference was positive and a learning moment.\nWith that, I hope to pay it forward by sharing some of what I learned at the conference.\nThe first two days of the conference were divided into 19 workshops, each taught from 9-5 for two days. I chose the Applied Machine Learning workshop in order to fill the gap in my knowledge about machine learning models beyond OLS and logistic regression. Max Kuhn and Davis Vaughn were the two workshop leaders and I knew they were in the process of developing the {tidymodels} ecosystem, which stands to be a successor to their popular {caret} package and promises fill the modeling gap in the {tidyverse} ecosystem. This was an amazing opportunity to both fill in the gaps as well as learn from the package developers themselves.\nMy notes for this workshop were incredibly sparse, in no small part because the workshop materials (which are free and available online from the workshop’s github repo) are very detailed.\nInstead of sharing those, I’ve decided to revisit a dataset we worked with during the workshop and present an example of a tidymodels workflow from start to finish, from sample splitting, to data preprocessing, to modeling, to tuning hyperparameters, to packaging it all up into a single workflow object.\nWe’ll be using the Ames Housing dataset which contains 81 variables and 2930 observations and our dependent variable is Sale_Price. Obviously, in an actual analysis we would spend much more time exploring this dataset, but for sole purpose of demonstrating the {tidymodels} workflow, we’ll just perform a variety of preprocessing, throw the kitchen sink at the data, then fit a Lasso model and a tuned elastic net model.\nLet’s start by inspecting the data.\nlibrary(tidymodels) library(AmesHousing) ames \u0026lt;- make_ames() ames %\u0026gt;% head() %\u0026gt;% knitr::kable()   MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape Land_Contour Utilities Lot_Config Land_Slope Neighborhood Condition_1 Condition_2 Bldg_Type House_Style Overall_Qual Overall_Cond Year_Built Year_Remod_Add Roof_Style Roof_Matl Exterior_1st Exterior_2nd Mas_Vnr_Type Mas_Vnr_Area Exter_Qual Exter_Cond Foundation Bsmt_Qual Bsmt_Cond Bsmt_Exposure BsmtFin_Type_1 BsmtFin_SF_1 BsmtFin_Type_2 BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF Heating Heating_QC Central_Air Electrical First_Flr_SF Second_Flr_SF Low_Qual_Fin_SF Gr_Liv_Area Bsmt_Full_Bath Bsmt_Half_Bath Full_Bath Half_Bath Bedroom_AbvGr Kitchen_AbvGr Kitchen_Qual TotRms_AbvGrd Functional Fireplaces Fireplace_Qu Garage_Type Garage_Finish Garage_Cars Garage_Area Garage_Qual Garage_Cond Paved_Drive Wood_Deck_SF Open_Porch_SF Enclosed_Porch Three_season_porch Screen_Porch Pool_Area Pool_QC Fence Misc_Feature Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price Longitude Latitude    One_Story_1946_and_Newer_All_Styles Residential_Low_Density 141 31770 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Corner Gtl North_Ames Norm Norm OneFam One_Story Above_Average Average 1960 1960 Hip CompShg BrkFace Plywood Stone 112 Typical Typical CBlock Typical Good Gd BLQ 2 Unf 0 441 1080 GasA Fair Y SBrkr 1656 0 0 1656 1 0 1 0 3 1 Typical 7 Typ 2 Good Attchd Fin 2 528 Typical Typical Partial_Pavement 210 62 0 0 0 0 No_Pool No_Fence None 0 5 2010 WD Normal 215000 -93.61975 42.05403  One_Story_1946_and_Newer_All_Styles Residential_High_Density 80 11622 Pave No_Alley_Access Regular Lvl AllPub Inside Gtl North_Ames Feedr Norm OneFam One_Story Average Above_Average 1961 1961 Gable CompShg VinylSd VinylSd None 0 Typical Typical CBlock Typical Typical No Rec 6 LwQ 144 270 882 GasA Typical Y SBrkr 896 0 0 896 0 0 1 0 2 1 Typical 5 Typ 0 No_Fireplace Attchd Unf 1 730 Typical Typical Paved 140 0 0 0 120 0 No_Pool Minimum_Privacy None 0 6 2010 WD Normal 105000 -93.61976 42.05301  One_Story_1946_and_Newer_All_Styles Residential_Low_Density 81 14267 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Corner Gtl North_Ames Norm Norm OneFam One_Story Above_Average Above_Average 1958 1958 Hip CompShg Wd Sdng Wd Sdng BrkFace 108 Typical Typical CBlock Typical Typical No ALQ 1 Unf 0 406 1329 GasA Typical Y SBrkr 1329 0 0 1329 0 0 1 1 3 1 Good 6 Typ 0 No_Fireplace Attchd Unf 1 312 Typical Typical Paved 393 36 0 0 0 0 No_Pool No_Fence Gar2 12500 6 2010 WD Normal 172000 -93.61939 42.05266  One_Story_1946_and_Newer_All_Styles Residential_Low_Density 93 11160 Pave No_Alley_Access Regular Lvl AllPub Corner Gtl North_Ames Norm Norm OneFam One_Story Good Average 1968 1968 Hip CompShg BrkFace BrkFace None 0 Good Typical CBlock Typical Typical No ALQ 1 Unf 0 1045 2110 GasA Excellent Y SBrkr 2110 0 0 2110 1 0 2 1 3 1 Excellent 8 Typ 2 Typical Attchd Fin 2 522 Typical Typical Paved 0 0 0 0 0 0 No_Pool No_Fence None 0 4 2010 WD Normal 244000 -93.61732 42.05125  Two_Story_1946_and_Newer Residential_Low_Density 74 13830 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Inside Gtl Gilbert Norm Norm OneFam Two_Story Average Average 1997 1998 Gable CompShg VinylSd VinylSd None 0 Typical Typical PConc Good Typical No GLQ 3 Unf 0 137 928 GasA Good Y SBrkr 928 701 0 1629 0 0 2 1 3 1 Typical 6 Typ 1 Typical Attchd Fin 2 482 Typical Typical Paved 212 34 0 0 0 0 No_Pool Minimum_Privacy None 0 3 2010 WD Normal 189900 -93.63893 42.06090  Two_Story_1946_and_Newer Residential_Low_Density 78 9978 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Inside Gtl Gilbert Norm Norm OneFam Two_Story Above_Average Above_Average 1998 1998 Gable CompShg VinylSd VinylSd BrkFace 20 Typical Typical PConc Typical Typical No GLQ 3 Unf 0 324 926 GasA Excellent Y SBrkr 926 678 0 1604 0 0 2 1 3 1 Good 7 Typ 1 Good Attchd Fin 2 470 Typical Typical Paved 360 36 0 0 0 0 No_Pool No_Fence None 0 6 2010 WD Normal 195500 -93.63893 42.06078    Tons of (probably) strongly related variables here. In the real world, we’d probably spend some time thinking about which variables are strictly necessary.\n Stratified training/test splits First, let’s split the data. Here, we’ll show one of the neat features of {rsample}, a package within the {tidymodels} ecosystem, which lets us perform stratified sampling on our dependent variable to ensure better balance. We’ll stick with the default split, which is a 75-25 Training-Test split.\n## Setting seed set.seed(1) ## Generate split ames_split \u0026lt;- initial_split(ames, strata = \u0026quot;Sale_Price\u0026quot;) ## Printing the function gives us \u0026lt;Num Rows in Training Set/Num Rows in Testing Set/Total Num Rows\u0026gt; ames_split ## \u0026lt;2199/731/2930\u0026gt; ## Calling training() on this object will give us our training set, and calling testing() on it will give us our testing set ames_train \u0026lt;- training(ames_split) ames_test \u0026lt;- testing(ames_split) ames_train %\u0026gt;% head() %\u0026gt;% knitr::kable()   MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape Land_Contour Utilities Lot_Config Land_Slope Neighborhood Condition_1 Condition_2 Bldg_Type House_Style Overall_Qual Overall_Cond Year_Built Year_Remod_Add Roof_Style Roof_Matl Exterior_1st Exterior_2nd Mas_Vnr_Type Mas_Vnr_Area Exter_Qual Exter_Cond Foundation Bsmt_Qual Bsmt_Cond Bsmt_Exposure BsmtFin_Type_1 BsmtFin_SF_1 BsmtFin_Type_2 BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF Heating Heating_QC Central_Air Electrical First_Flr_SF Second_Flr_SF Low_Qual_Fin_SF Gr_Liv_Area Bsmt_Full_Bath Bsmt_Half_Bath Full_Bath Half_Bath Bedroom_AbvGr Kitchen_AbvGr Kitchen_Qual TotRms_AbvGrd Functional Fireplaces Fireplace_Qu Garage_Type Garage_Finish Garage_Cars Garage_Area Garage_Qual Garage_Cond Paved_Drive Wood_Deck_SF Open_Porch_SF Enclosed_Porch Three_season_porch Screen_Porch Pool_Area Pool_QC Fence Misc_Feature Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price Longitude Latitude    One_Story_1946_and_Newer_All_Styles Residential_Low_Density 141 31770 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Corner Gtl North_Ames Norm Norm OneFam One_Story Above_Average Average 1960 1960 Hip CompShg BrkFace Plywood Stone 112 Typical Typical CBlock Typical Good Gd BLQ 2 Unf 0 441 1080 GasA Fair Y SBrkr 1656 0 0 1656 1 0 1 0 3 1 Typical 7 Typ 2 Good Attchd Fin 2 528 Typical Typical Partial_Pavement 210 62 0 0 0 0 No_Pool No_Fence None 0 5 2010 WD Normal 215000 -93.61975 42.05403  One_Story_1946_and_Newer_All_Styles Residential_Low_Density 81 14267 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Corner Gtl North_Ames Norm Norm OneFam One_Story Above_Average Above_Average 1958 1958 Hip CompShg Wd Sdng Wd Sdng BrkFace 108 Typical Typical CBlock Typical Typical No ALQ 1 Unf 0 406 1329 GasA Typical Y SBrkr 1329 0 0 1329 0 0 1 1 3 1 Good 6 Typ 0 No_Fireplace Attchd Unf 1 312 Typical Typical Paved 393 36 0 0 0 0 No_Pool No_Fence Gar2 12500 6 2010 WD Normal 172000 -93.61939 42.05266  One_Story_1946_and_Newer_All_Styles Residential_Low_Density 93 11160 Pave No_Alley_Access Regular Lvl AllPub Corner Gtl North_Ames Norm Norm OneFam One_Story Good Average 1968 1968 Hip CompShg BrkFace BrkFace None 0 Good Typical CBlock Typical Typical No ALQ 1 Unf 0 1045 2110 GasA Excellent Y SBrkr 2110 0 0 2110 1 0 2 1 3 1 Excellent 8 Typ 2 Typical Attchd Fin 2 522 Typical Typical Paved 0 0 0 0 0 0 No_Pool No_Fence None 0 4 2010 WD Normal 244000 -93.61732 42.05125  Two_Story_1946_and_Newer Residential_Low_Density 74 13830 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Inside Gtl Gilbert Norm Norm OneFam Two_Story Average Average 1997 1998 Gable CompShg VinylSd VinylSd None 0 Typical Typical PConc Good Typical No GLQ 3 Unf 0 137 928 GasA Good Y SBrkr 928 701 0 1629 0 0 2 1 3 1 Typical 6 Typ 1 Typical Attchd Fin 2 482 Typical Typical Paved 212 34 0 0 0 0 No_Pool Minimum_Privacy None 0 3 2010 WD Normal 189900 -93.63893 42.06090  Two_Story_1946_and_Newer Residential_Low_Density 78 9978 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Inside Gtl Gilbert Norm Norm OneFam Two_Story Above_Average Above_Average 1998 1998 Gable CompShg VinylSd VinylSd BrkFace 20 Typical Typical PConc Typical Typical No GLQ 3 Unf 0 324 926 GasA Excellent Y SBrkr 926 678 0 1604 0 0 2 1 3 1 Good 7 Typ 1 Good Attchd Fin 2 470 Typical Typical Paved 360 36 0 0 0 0 No_Pool No_Fence None 0 6 2010 WD Normal 195500 -93.63893 42.06078  One_Story_PUD_1946_and_Newer Residential_Low_Density 41 4920 Pave No_Alley_Access Regular Lvl AllPub Inside Gtl Stone_Brook Norm Norm TwnhsE One_Story Very_Good Average 2001 2001 Gable CompShg CemntBd CmentBd None 0 Good Typical PConc Good Typical Mn GLQ 3 Unf 0 722 1338 GasA Excellent Y SBrkr 1338 0 0 1338 1 0 2 0 2 1 Good 6 Typ 0 No_Fireplace Attchd Fin 2 582 Typical Typical Paved 0 0 170 0 0 0 No_Pool No_Fence None 0 4 2010 WD Normal 213500 -93.63379 42.06298     Data pre-processing Now let’s preprocess our data using the {recipes} package, also part of the {tidymodels} ecosystem. To do this, we’ll first specify our formula and our data, and then iterate the preprocessing steps we want. To demonstrate a wide range of things that can be done with {recipes}, let’s first log transform our dependent variable (Sale_Price), then remove variables containing \u0026quot;_Qual\u0026quot; or “Condition” (which are subjective ratings on the part of the appraiser made on or after the sale, we want to predict Sale_Price before sale!), create dummy variables out of our factor variables, center and scale our predictors, then run PCA on the 13 different variables that contain “SF” or “Area” to enough components to capture 75% of the variation in these variables, then remove any near-zero variance predictors. This all sounds like a lot, but the recipes package makes this pre-processing almost self-documenting!\names_rec \u0026lt;- recipe( Sale_Price ~ ., data = ames_train ) %\u0026gt;% step_log(Sale_Price, base = 10) %\u0026gt;% step_rm(matches(\u0026quot;Qual\u0026quot;), matches(\u0026quot;Cond\u0026quot;)) %\u0026gt;% step_dummy(all_nominal()) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors()) %\u0026gt;% step_pca(contains(\u0026quot;SF\u0026quot;), contains(\u0026quot;Area\u0026quot;), threshold = .75) %\u0026gt;% step_nzv(all_predictors()) ames_rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 80 ## ## Operations: ## ## Log transformation on Sale_Price ## Delete terms matches, Qual, matches, Cond ## Dummy variables from all_nominal ## Centering for all_predictors ## Scaling for all_predictors ## No PCA components were extracted. ## Sparse, unbalanced variable filter on all_predictors The next step is to prepare or prep() this recipe, which estimates any parameters necessary for the preprocessing steps from the training set to be later applied to other datasets.\names_rec_trained \u0026lt;- prep(ames_rec, training = ames_train, verbose = TRUE) ## oper 1 step log [training] ## oper 2 step rm [training] ## oper 3 step dummy [training] ## oper 4 step center [training] ## oper 5 step scale [training] ## oper 6 step pca [training] ## oper 7 step nzv [training] ## The retained training set is ~ 1.47 Mb in memory. ames_rec_trained ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 80 ## ## Training data contained 2199 data points and no missing data. ## ## Operations: ## ## Log transformation on Sale_Price [trained] ## Variables removed Overall_Qual, Exter_Qual, Bsmt_Qual, ... [trained] ## Dummy variables from MS_SubClass, MS_Zoning, Street, Alley, ... [trained] ## Centering for Lot_Frontage, Lot_Area, ... [trained] ## Scaling for Lot_Frontage, Lot_Area, ... [trained] ## PCA extraction with BsmtFin_SF_1, BsmtFin_SF_2, ... [trained] ## Sparse, unbalanced variable filter removed Kitchen_AbvGr, ... [trained] Now we can “juice” the prepared recipes, which gives us our preprocessed training set. Lets take a look at our PCA extraction.\names_rec_trained %\u0026gt;% juice() %\u0026gt;% select(starts_with(\u0026quot;PC\u0026quot;)) ## # A tibble: 2,199 x 7 ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -1.79 1.20 0.783 0.194 0.120 0.204 2.13 ## 2 -0.625 1.51 0.164 -0.0112 0.0919 1.50 0.256 ## 3 -2.57 -0.201 -0.677 0.723 1.08 -0.455 -0.799 ## 4 0.339 0.932 0.400 -0.230 -0.486 0.819 0.846 ## 5 0.159 0.903 0.385 -0.360 -0.390 1.62 0.510 ## 6 -0.00598 -0.337 -0.603 0.247 0.395 -0.0660 -0.781 ## 7 -0.172 -0.277 -0.689 -0.116 -0.219 -0.772 -0.838 ## 8 0.568 -1.19 0.716 -0.382 -0.347 0.243 0.715 ## 9 0.0123 1.80 0.152 -0.217 -0.0602 2.17 -0.0132 ## 10 1.06 -1.55 0.423 -0.211 -0.251 -0.520 0.318 ## # … with 2,189 more rows Not bad, we’ve reduced 13 variables down to 7. This probably wasn’t the best use case of PCA, but it provides a good example of some advanced preprocessing made simple in {recipes}.\n Modeling Now let’s specify our model. We’re going to go with a Lasso model with a penalty of 0.001 using the {parsnip} package.\nTo do this, we’re first going to specify our model as a linear regression using linear_reg(), set the mixture proportion to 1 for full L1 regularization (the Lasso), and the penalty to 0.001. Then we’ll set the engine to “glmnet”,as opposed to “lm”, “stan”, “spark”, or “keras” as alternative options. The beauty of {parsnip} is that it unifies the interface for model specifications so that you don’t need to remember dozens of different interfaces for each implementation of a model.\names_lasso \u0026lt;- linear_reg(penalty = 0.001, mixture = 1) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) Now we have our recipe and our model, we can create our “workflow”, which packages up our the preprocessing steps and model. Using workflows, we don’t need to go through the prep() and juice() steps we went through earlier when we go to fit our model (I demonstrated prep() and juice() as they can be useful for being able to inspect your pre-processed data as we did earlier).\names_lasso_wfl \u0026lt;- workflow() %\u0026gt;% add_recipe(ames_rec) %\u0026gt;% add_model(ames_lasso) ames_lasso_wfl ## ══ Workflow ══════════════════════════════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: linear_reg() ## ## ── Preprocessor ────────────────────────────────────────────────────────────────────────────────────────────────── ## 7 Recipe Steps ## ## ● step_log() ## ● step_rm() ## ● step_dummy() ## ● step_center() ## ● step_scale() ## ● step_pca() ## ● step_nzv() ## ## ── Model ───────────────────────────────────────────────────────────────────────────────────────────────────────── ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 0.001 ## mixture = 1 ## ## Computational engine: glmnet With our workflow designed, fitting our model is as simple as passing our training data and workflow to the fit() function.\names_lasso_fit \u0026lt;- fit(ames_lasso_wfl, ames_train) And getting predictions is as simple as passing out fitted model and the data we want predictions for to the predict() function.\npredict(ames_lasso_fit, ames_train) %\u0026gt;% slice(1:5) ## # A tibble: 5 x 1 ## .pred ## \u0026lt;dbl\u0026gt; ## 1 5.32 ## 2 5.16 ## 3 5.36 ## 4 5.30 ## 5 5.32  Model evaluation How does our model perform on our training set? Let’s find out using metrics from the {yardstick} package. We’ll use three metrics: Root Mean Squared Error (RMSE), R squared, and the concordance correlation coefficient (ccc).\nFirst we’ll set our three metrics, then we’ll generate predictions, and compare those predictions to the true values within the training set.\nperf_metrics \u0026lt;- metric_set(rmse, rsq, ccc) perf_lasso \u0026lt;- ames_lasso_fit %\u0026gt;% predict(ames_train) %\u0026gt;% bind_cols(juice(ames_rec_trained)) %\u0026gt;% perf_metrics(truth = Sale_Price, estimate = .pred) perf_lasso %\u0026gt;% arrange(.metric) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ccc standard 0.924 ## 2 rmse standard 0.0657 ## 3 rsq standard 0.861 Easy peasy! But of course, this is all in-sample. Perhaps we want to know what kind of out-of-sample performance we can expect from our model using cross-validation. {rsample} also makes that easy, so let’s create 10-fold cross-validation sets for evaluating our training set models using vfold_cv(), which defaults to creating 10 folds.\ncv_splits \u0026lt;- vfold_cv(ames_train) cv_splits ## # 10-fold cross-validation ## # A tibble: 10 x 2 ## splits id ## \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026lt;split [2K/220]\u0026gt; Fold01 ## 2 \u0026lt;split [2K/220]\u0026gt; Fold02 ## 3 \u0026lt;split [2K/220]\u0026gt; Fold03 ## 4 \u0026lt;split [2K/220]\u0026gt; Fold04 ## 5 \u0026lt;split [2K/220]\u0026gt; Fold05 ## 6 \u0026lt;split [2K/220]\u0026gt; Fold06 ## 7 \u0026lt;split [2K/220]\u0026gt; Fold07 ## 8 \u0026lt;split [2K/220]\u0026gt; Fold08 ## 9 \u0026lt;split [2K/220]\u0026gt; Fold09 ## 10 \u0026lt;split [2K/219]\u0026gt; Fold10 Now we’ll take our workflow and use it to fit 10 models on these 10 splits using the fit_resamples() function from the {tune} package (also a part of the tidymodels ecosystem), as well as tell it to compute the performance metrics we set earlier.\ncv_eval \u0026lt;- fit_resamples(ames_lasso_wfl, resamples = cv_splits, metrics = perf_metrics) cv_eval ## # 10-fold cross-validation ## # A tibble: 10 x 4 ## splits id .metrics .notes ## * \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [2K/220]\u0026gt; Fold01 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; ## 2 \u0026lt;split [2K/220]\u0026gt; Fold02 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; ## 3 \u0026lt;split [2K/220]\u0026gt; Fold03 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; ## 4 \u0026lt;split [2K/220]\u0026gt; Fold04 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; ## 5 \u0026lt;split [2K/220]\u0026gt; Fold05 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; ## 6 \u0026lt;split [2K/220]\u0026gt; Fold06 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; ## 7 \u0026lt;split [2K/220]\u0026gt; Fold07 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; ## 8 \u0026lt;split [2K/220]\u0026gt; Fold08 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; ## 9 \u0026lt;split [2K/220]\u0026gt; Fold09 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; ## 10 \u0026lt;split [2K/219]\u0026gt; Fold10 \u0026lt;tibble [3 × 3]\u0026gt; \u0026lt;tibble [0 × 1]\u0026gt; Now let’s compare the in-sample performance we just checked to our cross-validated performance, which is as easy as passing the above fit_samples() object to the collect_metrics() function!\ncollect_metrics(cv_eval) ## # A tibble: 3 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ccc standard 0.911 10 0.00767 ## 2 rmse standard 0.0686 10 0.00289 ## 3 rsq standard 0.847 10 0.0155 perf_lasso %\u0026gt;% arrange(.metric) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ccc standard 0.924 ## 2 rmse standard 0.0657 ## 3 rsq standard 0.861 Not bad at all! Our cross-validated performance is fairly close to our in-sample performance, it doesn’t seem like we’re overfitting here.\nBut I think we can do better. We’ve already used the {tune} package to fit across these resamples, but as the package name suggests, its real power comes in allowing us to easily tune the hyperparameters in our model.\n Model tuning Recall that we set our regularization penalty to 0.001 and we chose to use L1 regularization. Both those decisions were relatively arbitrary. Let’s use the {tune} package to build an elastic net model by exploring other penalty values and regularization mixtures using cross validation performance to decide what values these parameters should be.\nWe’ll start by defining a new model, ames_mixture, which will not take specific specific values for penalty and mixture and will instead leave these as variables to be tuned and swapping out our ames_lasso_wfl with this new workflow.\names_mixture \u0026lt;- linear_reg(penalty = tune(), mixture = tune()) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) ames_mixture_wfl \u0026lt;- update_model(ames_lasso_wfl, ames_mixture) Next, we will define a parameter space to search. {tune} allows you to perform either grid search (where the candidate values are pre-defined) or iterative search (ex: Bayesian optimization) where the results of the previous model are used to select the next parameter values to try.\nThere are pros/cons to each. A big plus of grid search is that it allows you to take advantage of parallel processing to speed up your search, while iterative search is, by construction, sequential. A big plus of iterative search is that it can quickly rule out areas of parameter space which can be efficient when covering many values of a high dimensional parameter space (where a grid may require many, many models to comfortably cover the entire parameter space, where many of them may turn out to be redundant).\nFor this post, we’re going to stick with grid search. The simplest form of grid search uses regular grids, where you provide a vector of values for each parameter and the grid is composed of every possible value combination.\n{tune} provides useful defaults for searching parameter spaces of many common hyperparameters, for example, creating grids for the “penalty” parameter in log-10 space. We can simply specify the parameters, pass these to grid_regular(), and specify that we want 5 levels of penalization and 5 levels of mixture.\nmixture_param \u0026lt;- parameters(penalty(), mixture()) regular_grid \u0026lt;- grid_regular(mixture_param, levels = c(5, 5)) regular_grid %\u0026gt;% ggplot(aes(x = mixture, y = penalty)) + geom_point() + scale_y_log10() {tune} also provides ways to create non-regular grids as well.\n Random grids generated using grid_random() will uniformly sample the parameter space.\n Space-filling designs (SFD) generated using grid_max_entropy() will try to keep candidate values away from one another in order to more efficiently cover the parameter space.\n  The below shows how to create a SFD grid and plots 25 candidate values.\nsfd_grid \u0026lt;- grid_max_entropy(mixture_param, size = 25) sfd_grid ## # A tibble: 25 x 2 ## penalty mixture ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1.21e- 8 0.614 ## 2 1.32e- 1 0.679 ## 3 1.20e-10 0.269 ## 4 3.90e- 9 0.414 ## 5 8.38e- 6 0.0756 ## 6 4.02e- 1 0.838 ## 7 5.94e-10 0.504 ## 8 1.83e- 1 0.177 ## 9 3.61e- 3 0.866 ## 10 2.86e- 7 0.856 ## # … with 15 more rows sfd_grid %\u0026gt;% ggplot(aes(x = mixture, y = penalty)) + geom_point() + scale_y_log10() For simplicity’s sake, we’ll stick with the regular grid we generated. Let’s start tuning.\nFirst, we’ll set up our parallelization.\nlibrary(doParallel) all_cores \u0026lt;- parallel::detectCores(logical = FALSE) cl \u0026lt;- makePSOCKcluster(all_cores) registerDoParallel(cl) clusterEvalQ(cl, {library(tidymodels)}) Now we’re going to create our tuning object, which will take our recipe, our model, our resamples, and our metrics, to fit our 25 models over 10 resamples and compute our performance metrics, then we’ll stop our parallelization.\names_tune \u0026lt;- tune_grid( ames_rec, model = ames_mixture, resamples = cv_splits, grid = regular_grid, metrics = perf_metrics ) stopCluster(cl) # Naive Lasso performance collect_metrics(cv_eval) ## # A tibble: 3 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ccc standard 0.911 10 0.00767 ## 2 rmse standard 0.0686 10 0.00289 ## 3 rsq standard 0.847 10 0.0155 # Best tuned models show_best(ames_tune, \u0026quot;ccc\u0026quot;) ## # A tibble: 5 x 7 ## penalty mixture .metric .estimator mean n std_err ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.0000000001 0.25 ccc standard 0.913 10 0.00760 ## 2 0.0000000316 0.25 ccc standard 0.913 10 0.00760 ## 3 0.00001 0.25 ccc standard 0.913 10 0.00760 ## 4 0.0000000001 0.75 ccc standard 0.913 10 0.00762 ## 5 0.0000000316 0.75 ccc standard 0.913 10 0.00762 show_best(ames_tune, \u0026quot;rmse\u0026quot;, maximize = FALSE) ## # A tibble: 5 x 7 ## penalty mixture .metric .estimator mean n std_err ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.0000000001 0 rmse standard 0.0682 10 0.00235 ## 2 0.0000000316 0 rmse standard 0.0682 10 0.00235 ## 3 0.00001 0 rmse standard 0.0682 10 0.00235 ## 4 0.00316 0 rmse standard 0.0682 10 0.00235 ## 5 0.00316 0.25 rmse standard 0.0684 10 0.00274 show_best(ames_tune, \u0026quot;rsq\u0026quot;) ## # A tibble: 5 x 7 ## penalty mixture .metric .estimator mean n std_err ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.0000000001 0 rsq standard 0.849 10 0.0130 ## 2 0.0000000316 0 rsq standard 0.849 10 0.0130 ## 3 0.00001 0 rsq standard 0.849 10 0.0130 ## 4 0.00316 0 rsq standard 0.849 10 0.0130 ## 5 0.00316 0.25 rsq standard 0.848 10 0.0148 Our results suggest that our original model parameters choices definitely had room for improvement. A much smaller penalty and going with pure L2 regularization seems to perform better on this data. The improvements are relatively modest (RMSE: 0.0686 –\u0026gt; 0.0682, R squared: 0.847 –\u0026gt; 0.849), but when tuning is this easy, why leave money on the table?\nThe plot below nicely visualizes the performance of each grid candidate nicely, along with a dotted line to indicate where our original model would’ve been.\ncollect_metrics(ames_tune) %\u0026gt;% filter(.metric == \u0026quot;rmse\u0026quot;) %\u0026gt;% mutate(mixture = format(mixture)) %\u0026gt;% ggplot(aes(x = penalty, y = mean, col = mixture)) + geom_line() + geom_point() + scale_x_log10() + geom_vline(xintercept = 0.001, color = \u0026quot;purple\u0026quot;, lty = \u0026quot;dotted\u0026quot;) Now let’s select our best grid candidate, finalize our workflow, and fit our model.\nbest_mixture \u0026lt;- select_best(ames_tune, metric = \u0026quot;rmse\u0026quot;, maximize = FALSE) best_mixture ## # A tibble: 1 x 2 ## penalty mixture ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.0000000001 0 ames_mixture_final \u0026lt;- ames_mixture_wfl %\u0026gt;% finalize_workflow(best_mixture) %\u0026gt;% fit(data = ames_train) And we’re done! We now have a fitted, tuned, regularized mixtured model (albeit, the mixture is 100% L2 regularization, but we got there via tuning!)\nFinally we get to the fun part. What variables turned out to be the most important in predicting sale price?\ntidy_coefs \u0026lt;- ames_mixture_final$fit$fit$fit %\u0026gt;% broom::tidy() %\u0026gt;% filter(term != \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% select(-step, -dev.ratio) delta \u0026lt;- abs(tidy_coefs$lambda - best_mixture$penalty) lambda_opt \u0026lt;- tidy_coefs$lambda[which.min(delta)] label_coefs \u0026lt;- tidy_coefs %\u0026gt;% mutate(abs_estimate = abs(estimate)) %\u0026gt;% filter(abs_estimate \u0026gt;= 0.01) %\u0026gt;% distinct(term) %\u0026gt;% inner_join(tidy_coefs, by = \u0026quot;term\u0026quot;) %\u0026gt;% filter(lambda == lambda_opt) label_coefs ## # A tibble: 10 x 3 ## term estimate lambda ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Garage_Cars 0.0156 0.0139 ## 2 Year_Remod_Add 0.0235 0.0139 ## 3 TotRms_AbvGrd 0.0134 0.0139 ## 4 Full_Bath 0.0124 0.0139 ## 5 PC1 -0.0244 0.0139 ## 6 Fireplaces 0.0121 0.0139 ## 7 Central_Air_Y 0.0121 0.0139 ## 8 Bsmt_Full_Bath 0.0103 0.0139 ## 9 Neighborhood_Gilbert -0.0110 0.0139 ## 10 MS_Zoning_Residential_Low_Density 0.0107 0.0139 tidy_coefs %\u0026gt;% ggplot(aes(x = lambda, y = estimate, group = term, col = term, label = term)) + geom_vline(xintercept = lambda_opt, lty = 3) + geom_line(alpha = .4) + theme(legend.position = \u0026quot;none\u0026quot;) + scale_x_log10() + ggrepel::geom_text_repel(data = label_coefs) The above shows the coefficient estimates plotted against lambda, the dotted line indicating the optimal lambda that we selected during our tuning. Nice to see that one of our principal components ended up being important!\nWith that all said and done, let’s finally see how our model did against our test set.\names_mixture_final %\u0026gt;% predict(ames_test) %\u0026gt;% bind_cols(select(ames_test, Sale_Price)) %\u0026gt;% mutate(Sale_Price = log10(Sale_Price)) %\u0026gt;% perf_metrics(truth = Sale_Price, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 rmse standard 0.0828 ## 2 rsq standard 0.787 ## 3 ccc standard 0.877 Having practiced on this data before, I can say that this is not great performance, but as mentioned before, all of this is just to demonstrate {tidymodels} functionality and workflow.\nThere are many, many more features to the {tidymodels} ecosystem and more are continually being developed. I encourage you to explore the vignettes for its composite packages. The parent website is not live as of the writing of this post, but I expect it will be soon. In the mean time, the tidymodels github repository can point you to the vignettes for each of its composite packages.\n ","date":1580860800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580860800,"objectID":"c0d160abb80fc67abb0b2da022a7992b","permalink":"https://dnield.com/posts/tidymodels-intro/","publishdate":"2020-02-05T00:00:00Z","relpermalink":"/posts/tidymodels-intro/","section":"posts","summary":"Lessons from the Applied Learning Workshop at rstudio::conf(2020)","tags":["rstats","data science","machine learning"],"title":"Introduction to the {tidymodels} Machine Learning Ecosystem","type":"posts"},{"authors":null,"categories":["Kaggle"],"content":" Demographics The data field today is as disproportionately male as its ever been. No clear trend in age composition The increasingly educated data workforce  Titles and Compensation The ascendency of the “data scientist” title Data scientists’ compensation has increased even as the title has become more widespread  Languages Python’s meteoric rise  Wrapping up   Kaggle has released the data for their third annual Machine Learning and Data Science Survey. I’ve only recently joined the Kaggle platform as I’ve transitioned from academia to private industry, so this seems to be an excellent opportunity to explore the backgrounds of my new data science peers.\nThis is the second blog post exploring this data.\nPart 1 explored this year’s survey results.\nPart 2, this post, brings in survey data from the first two years of this annual survey to investigate how the field has changed over the last three years in the United States.\nNote: Because I am based in the United States, as are most of the data science community with which I interact with regularly in-person or on social media, this analysis is limited to the 3085 survey respondents living in the United States.\nLet’s get started!\nDemographics The data field today is as disproportionately male as its ever been. Perhaps unsurprising given how disportionately male we found the field to be in part 1, but there appears to be no trend towards increasing gender diversity among data professionals.\n No clear trend in age composition Survey data doesn’t seem to show a clear trend in the age composition of data professionals. This isn’t to say that the composition is stable or unchanging. As always, it should be noted that this survey is a not a random sample of data professionals, but a voluntary response sample survey of Kaggle users, so the composition may change widely based on how Kaggle chooses to promote the survey.\n The increasingly educated data workforce The plot above shows the highest education attainment of respondents to all three years’ surveys. The number of data professionals holding a Master’s degree shocked me in part 1, but the data show that this has been an ongoing trend, while the percent of data professionals holding professional degrees, or only some college education or a high school diploma is near zero. Whether this trend towards a more educated data workforce is due to former Bachelor’s holders seeking and attaining higher education or due to new hires disporportionately coming out of grad school is unclear.\n  Titles and Compensation The ascendency of the “data scientist” title The proportion of data professionals on Kaggle with the job title “data scientist” has increased 30% relative to 2017: from 30% to 40%. And this doesn’t seem to be simple title changing from former “data analysts”, who also have increased as a proportion of employed Kaggle respondents.\nThe greatest decline in relative share of the data workforce are among engineers (which include all titles with “engineer” in their title, from data engineers to SWEs) and researchers (which include all titles with “research” in their title, with the exception of research assistants, which were excluded because these positions are typically not careers).\nMore difficult to see here is the relative decline in the “Statistician” title, which started at a barely registerable 3.3%, but has since fallen to 2.4%.\n Data scientists’ compensation has increased even as the title has become more widespread The above plot reports the typical (median) compensation of respondents with each job title. The responses are an ordinal factor so the data is an idomatic median: arrange by the ordered factor, and take the median value. Given a vector of even length and the middle two values are two different categories (e.g. $80,000-89,999 and $90,000-99,999), the lower value will be used.\nThe function for this implementation is below, with credit to Hong Ooi and Richie Cotton from StackOverflow.\nmedian.ordered \u0026lt;- function(x) { levs \u0026lt;- levels(x) m \u0026lt;- median(as.integer(x), na.rm = TRUE) if(floor(m) != m) { m \u0026lt;- floor(m) } ordered(m, labels = levs, levels = seq_along(levs)) } On the whole, the wages of data professionals appear to be on the rise. The median compensation for respondents of every job title except for Engineers appears to be higher in 2019 than it was in 2017 (and median Engineer compensation remaining steady at a very respectable $100,000 to $125,000). Data scientists in particular have broken away to typically make $125,000 to $150,000.\n  Languages Python’s meteoric rise About two-thirds of data professionals who program today are using Python, this is a 50% increase relative to the proportion using Python in 2017. Likewise, SQL use has increased 30% during the same time, while R use has remained fairly stable at about 33% and all other languages stable at about 10%.\n  Wrapping up So there you have it.\nAs far as I see here, here are the three big take-aways from Kaggle’s three years of survey results:\n Unfortunately, the data workforce does not appear to becoming any more gender diverse than it was in 2017.\n “Data scientist” as a title has become extremely widespread, and in spite of this proliference, is as highly paid as ever.\n Python and SQL are becoming universal, but not at the expense of other languages.\n  \n","date":1575936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575936000,"objectID":"fea4294a4646ea334299ed618f618749","permalink":"https://dnield.com/posts/kaggle-survey/pt-two/","publishdate":"2019-12-10T00:00:00Z","relpermalink":"/posts/kaggle-survey/pt-two/","section":"posts","summary":"How has the data science landscape changed over the last three years?","tags":["survey analysis","data visualization","data science"],"title":"The State of Data Science (Part 2)","type":"posts"},{"authors":null,"categories":["Kaggle"],"content":" Demographics Three-quarters of data professionals are men Data professionals are fairly young Underrepresentation of women persists across cohort The majority of data professionals have an advanced degree  Titles, Compensation, Experience, and Roles “My Name is Scientist, Data Scientist” Data scientists make the big bucks Data professionals’ team sizes are highly bimodal Data professionals’ most common important role is informing business decisions  Languages and Tools Python is King The majority of data professionals on Kaggle are relatively new to writing code for data analysis. There’s a shocking amount of experienced machine learning practitioners!  That’s all, folks!   Kaggle has released the data for their third annual Machine Learning and Data Science Survey. I’ve only recently joined the Kaggle platform as I’ve transitioned from academia to private industry, so this seems to be an excellent opportunity to explore the backgrounds of my new data science peers.\nThis is the first of the two part series exploring this data.\nToday we will explore this year’s survey results.\nPart 2 will bring in survey data from the first two years of this annual survey to investigate how the field has changed over the last three years in the United States.\nNote: Because I am based in the United States, as are most of the data science community with which I interact with regularly in-person or on social media, this analysis is limited to the 3085 survey respondents living in the United States.\nLet’s get started!\nDemographics Three-quarters of data professionals are men I’m not surprised that the majority of data professionals are men, but I was shocked at how stark the split is. At Callisto (my company), our R\u0026amp;D team (which includes analysts and engineers) is almost exactly evenly split, which I knew is an outlier in the Data industry, but I didn’t realize by how much.\n Data professionals are fairly young As one might expect from a field that itself is relatively young, data professionals themselves are fairly young, with the modal data professional being age 25 to 29. I, myself, am turning 25 next month, so I’m pretty much modal here.\nBe wary here, as the bin widths of these ages are changing across the x-axis. Unfortunately, this is as specific as the data get.\nOne alternative explanation for these data is that many Kagglers are early career data professionals who use or used Kaggle to build a professional portfolio for the job market. Older data professionals may be underrepresented here.\n Underrepresentation of women persists across cohort I expected there to be underrepresentation of women in the Data industry, but I would’ve expected the divide to be much less stark among younger cohorts of data professionals than older ones. The above plots show that this doesn’t appear to be the case.\nThe top panel shows a representation of a contingency table, that is, the percent of respondents that identify as male or female that fall into each age category. Women are clearly underrepresented at every age.\nThe bottom two panels break this plot into its marginals. The left panel shows the percent of data professionals of each gender that fall into each age category. It shows the same broad pattern for both men and women, with female data professionals skewing just a bit younger. The right shows the gender split of each age category. It shows that the gender split of data professionals hovers around 75% for almost every cohort except those over 60 years old. Clearly the underrepresentation of women in the data industry is not something that will go away as a result of retiring older cohorts.\n The majority of data professionals have an advanced degree Unsurprisingly, data professionals are an educated bunch. Surprising to me, however, was the number of Master’s degree holders and the relative sparsity of PhDs and professional degree holders. Coming from academia, I expected the data industry to be primarily comprised of: 1) non-CS Bachelor’s and MBA holders building reports and dashboards, 2) CS/Engineering Bachelor’s holders building and maintaining data infrastructure, and 3) PhDs who left academia due to the job market or lured by mobility and money in private industry. I did not expect to see that half of all data professionals are Master’s holders!\n  Titles, Compensation, Experience, and Roles “My Name is Scientist, Data Scientist” Another shocking finding to me was sheer amount of data scientists in the survey relative to all other roles. I have a pretty strong Bayesian prior that there aren’t 2.5x as many data scientists as there are data analysts in the data industry, so this is definitely a reality check that this survey should not be considered a representative survey of the industry.\nAdditionally, the sheer number of students represented in the survey seems consistent with my hypothesis that many Kagglers may be on the platform to build up an data analysis portfolio for the job market. The percentage (3.6%) of “Not Employed” respondents seems consistent with this as well.1 I suspect the true unemployment rate for data professionals is, in reality, lower than the national unemployment rate (3.7% as of this writing), not almost identical to it.2\n Data scientists make the big bucks The 2019 Kaggle ML \u0026amp; DS Survey reports annual compensation in brackets. I’m interested in the median income of data professionals by title, but obviously taking the median of categorical data isn’t something R takes kindly to!\nHowever, since this factor is ordinal, it is possible to take an idomatic median: just arrange by the ordered factor, and take the median value. The hitch is when you have a vector of even length and the middle two values are two different categories (e.g. $80,000-89,999 and $90,000-99,999). Strictly speaking, I prefer to just take the cutpoint between the two categories ($90,000) or an interval with the cutpoint as the center point as the value. However, for this quick and dirty exploration, I’m just going to take the lower of the two categories as the winner of the tie.\nThe function for this implementation is below, with credit to Hong Ooi and Richie Cotton from StackOverflow.\nmedian.ordered \u0026lt;- function(x) { levs \u0026lt;- levels(x) m \u0026lt;- median(as.integer(x), na.rm = TRUE) if(floor(m) != m) { m \u0026lt;- floor(m) } ordered(m, labels = levs, levels = seq_along(levs)) } The above plot shows the median income of each job title computed in this way, preserving the ordering of job titles by their frequency in the previous plot. Students and Unemployed respondents were not asked for their annual compensation, but in order to preserve the frequency order they are represented here.\nClearly, as a Data Analyst, I’m in the wrong job! Data Analysts and Business Analysts are tied for last, with the median analysts making $80,000 to $89,999 annually, while Data Scientists make the big bucks, with the median data scientist making $125,000 to $149,999 annually.\n Data professionals’ team sizes are highly bimodal Almost 40% of business have data science teams of less than five while another 40% have data science teams of over twenty! That’s quite the bimodality. Hard to think of an explanation about why this would be, although my intuition is that this is due to how young the field is combined with restricted\n Data professionals’ most common important role is informing business decisions I’m a bit disappointed in the possible multiple-choice answers here. Machine learning spans three of the five possible roles, however building, improving, and researching machine learning models, I would argue, is something that more generalist data professionals engage in, even when they have the skills, meanwhile “influence business decisions” and “build or run data infrastructure” are so broad as to be almost uninformative, while the distinction between “building” and “improving” ML models seems much less useful than knowing the type of models being used (predictive or forecasting models vs classification models vs generative models).\nI would have liked to see response options broken out into “building and/or maintaining periodic reports or dashboards”, “experimental design”, “forecasting”, etc.\nNonetheless, the key finding here is that the most common important role of data professionals appears to be informing business decisions.\n  Languages and Tools Python is King Unfortunately, Julia wasn’t offered as an option on this survey, and I’m not sure why. It was asked about in previous iterations of this survey, and Julia has continued to mature as a language.\nMuch to my chagrin as an R user, Python dominates as the most coding language used by data professionals. SQL comes next, followed by R, then Java, followed by a long tail of other languages.\nI’m surprised by the proliference of Java. I’ve always thought of the modal data professional toolkit as: SQL for querying databases, R/Python for modeling and prototyping, and C++ for production code. I don’t know of any data scientists using Java. I’ll be curious to dig into this more.\n The majority of data professionals on Kaggle are relatively new to writing code for data analysis. The majority of data professionals have less than five years of experience writing code for data analysis, which is consistent with the broadly young age distribution of data professionals. Like with age, my four years experience writing code for analysis make me pretty modal here as well.\nI’m curious about the typical experience levels of the job titles earlier. Let’s taking the same idiomatic median we used earlier to compute median income to see what the median experience category is for each job title.\nPerhaps unsurprisingly, data scientists, research scientists, data engineers, and statisticians clock in as the most experience writing code to analyze data, while students are the least experienced. It appears that my level of coding experience is typical for my job title among Kagglers.\n There’s a shocking amount of experienced machine learning practitioners! First, it should be noted that this question was only asked of those that said they had experience coding data analysis. I should note that I believe that the “\u0026lt; 1 years” category includes data professionals with less than a year of ML experience with those with none. Kaggle offered the survey schema and question wording for this survey, but did not provide a list of all of the possible responses to each multiple-choice question. In previous years of this survey, “I have never studied machine learning but plan to learn in the future” was an offered response option. However, not a single response among the over 17,000 respondents in the 2019 survey indicates no experience, which seems unlikely to happen if no experience was an option.3 It seems more likely that “less than 1 years experience” captures both respondents with no experience and those with less than some number of months experience.\nNonetheless, even discounting this to the maximum extent possible (that all \u0026lt; 1 year responses indicate zero experience), the amount of ML experience in the community surprised me quite a bit.\nFirst of all, there’s no reason ex-ante that data analysis should entail fitting a model to data. Plenty of (I’d argue most) excellent analysis can be done through simple graphs and tables so there’s no strong reason to need it.\nSecondly, I would be surprised if every single Kaggler has the training (formal or informal) necessary for the application and interpretation of even simple machine learning models such as linear regression, nevermind more complex “black box” models like neural networks and support vector machines.\nThirdly, the very definition of what counts as “machine learning” is contentious. Does linear regression even count? My gut says no, only the related linear model selection algorithms like MARS, lasso, and ridge regression count, but linear regression is considered to be a machine learning algorithm by many others4 and is listed as a machine learning algorithm in this survey. By my own definition, I only have 2 years of machine learning experience, but if linear regression counts then I have 4 years experience, which, according this data, makes me a veteran!\nLuckily, another question asks respondents which machine learning algorithms they use on a regular basis, helping us to dig into this a bit more.\nAs expected, linear and logistic regression top the charts as the algorithms used on a regular basis by the most data professionals. Still, I’m surprised by how popular decision tree models and gradient boosting machines are. Perhaps a testament to how easy out-of-the-box versions of these algorithms have become to deploy due to the maturity of packages like sklearn, xgboost, caret, ranger, randomForest, among others.\nThe ecology of machine learning platforms has developed to the degree that the practioners dilemma seems to be deciding which of the numerous high quality frameworks and packages to use.\nLuckily, we have data on this as well!\nWith Python dominance comes sklearn dominance. Luckily for R users, TensorFlow and Keras remain the dominant framework for building neural networks. I expect PyTorch to continue to grow, but if it ever overtakes TensorFlow, I expect R implementations and interfaces to have matured by then.\nI also expect that the tidymodels framework, developed by Max Kuhn, the author of “caret”, and other authors in the tidyverse to succeed caret to begin emerging as a leading machine learning framework for R users in 2020 as the ecosystem continues to mature.\n  That’s all, folks! As mentioned before, Part 2 of this series will be a comparing the results of this survey to the results of the two prior annual Kaggle surveys to see how the industry has changed over the last three years.\nExpect that post whenever I get around to cleaning those (far larger, in terms of questionnaire length) survey datasets.\n  Retired respondents are not counted as Not Employed here, as they were instructed to choose the most similar to the one they held most recently↩\n Some readers with an economics background may, rightfully, chime in that some frictional unemployment may actually be a sign of a healthy labor market that favors workers, as people may quit jobs knowing they’ll get a better job shortly, although I would argue that this is still consistent with the point that Kagglers are on the platform, in part, as a signal to employers↩\n Even if it’s true that every single respondent has some experience with machine learning, even a miniscule amount of measurement error should result in at least one response indicating none↩\n Including Hastie et al in Elements of Statistical Learning, one of the many bibles on the topic↩\n  \n","date":1574121600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574121600,"objectID":"f3a7f8b143c8c3fd8052a1940b23815f","permalink":"https://dnield.com/posts/kaggle-survey/pt-one/","publishdate":"2019-11-19T00:00:00Z","relpermalink":"/posts/kaggle-survey/pt-one/","section":"posts","summary":"What does the data science landscape look like today?","tags":["survey analysis","data visualization","data science"],"title":"The State of Data Science (Part 1)","type":"posts"},{"authors":["David Nield"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://dnield.com/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://dnield.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://dnield.com/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"About / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"6f8c6b33263a59bd4e4de690decef4a9","permalink":"https://dnield.com/projects/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/projects/external-project/","section":"projects","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"projects"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"79cfc0c3ddcfb9255d2520d5f850143e","permalink":"https://dnield.com/projects/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/projects/internal-project/","section":"projects","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"projects"},{"authors":["David Nield","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://dnield.com/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["David Nield","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://dnield.com/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]